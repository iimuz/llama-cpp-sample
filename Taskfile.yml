version: "3"

vars: {}
dotenv: [".env"]
env: {}

tasks:
  # 初期環境構築用タスク
  init:
    desc: Initialize project.
    deps:
      - clean
    cmds:
      - git submodule init
      - git submodule update
      - task: build
  clean:
    desc: Clean files.
    cmds:
      - cd llama.cpp && git clean -xdf .

  build:
    desc: Build llama.cpp.
    dir: llama.cpp
    cmds:
      # - task: build-cuda
      - task: build-mac
  build-cuda:
    desc: Build llama.cpp.
    dir: llama.cpp
    cmds:
      - make -j4 LLAMA_CUDA=1
  build-mac:
    desc: Build llama.cpp using METAL.
    dir: llama.cpp
    platform: ["darwin"]
    cmds:
      # デフォルトで有効のため追加設定は不要
      #
      # > On MacOS, Metal is enabled by default. Using Metal makes the computation run on the GPU.
      # > <https://github.com/ggerganov/llama.cpp>
      - make -j4

  download-model:
    desc: Download model.
    dir: llama.cpp/models
    cmds:
      - wget https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf?download=true

  inference:
    desc: Inference.
    dir: llama.cpp
    cmds:
      - './main -m models/llama3.gguf --temp 0.1 -ngl 32 -b 512 -p "### Instruction: What is the height of Mount Fuji? ### Response:"'
